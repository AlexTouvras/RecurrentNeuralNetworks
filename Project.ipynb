{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tIntroduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text mining and NLP problems (IMDB reviews dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tThis project will explore and analyze the information stored in a particular dataset. In this case the IMDB reviews dataset (https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). We will explore different RNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will deliver three different tasks:\n",
    "1. Implement the selected model.\n",
    "2. Tuning the presented models and describing how and why the results get improvement.\n",
    "3. Compare the final model with one other RNN model. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alext\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define all and only used package imports below\n",
    "#!pip install opendatasets\n",
    "import opendatasets as od #load kaggle datasets\n",
    "import pandas as pd \n",
    "#!pip install nltk\n",
    "from nltk.corpus import stopwords # to calculate the number of stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tELT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract, Load and Transform of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data should be retrieved from an online source, NOT from your local drive, otherwise, nobody can run your code without additional effort.\n",
    "- Check the following link on how to retrieve data from kaggle https://www.analyticsvidhya.com/blog/2021/04/how-to-download-kaggle-datasets-using-jupyter-notebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \".\\imdb-dataset-of-50k-movie-reviews\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# Download data from kaggle\n",
    "od.download(\"https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "# Read the downloaded file\n",
    "data = pd.read_csv('.\\imdb-dataset-of-50k-movie-reviews\\IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the essential description of data.\n",
    "-\tDon’t print out dozens of raw lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>231.145940</td>\n",
       "      <td>171.326419</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2470.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_count</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>1309.431020</td>\n",
       "      <td>989.728014</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>970.000000</td>\n",
       "      <td>1590.250000</td>\n",
       "      <td>13704.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_word</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>4.640676</td>\n",
       "      <td>0.340731</td>\n",
       "      <td>1.239865</td>\n",
       "      <td>4.417904</td>\n",
       "      <td>4.627006</td>\n",
       "      <td>4.847458</td>\n",
       "      <td>12.290909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopwords</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>94.768780</td>\n",
       "      <td>71.682687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>1004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>4.857800</td>\n",
       "      <td>5.627519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>162.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count         mean         std        min         25%  \\\n",
       "word_count  50000.0   231.145940  171.326419   4.000000  126.000000   \n",
       "char_count  50000.0  1309.431020  989.728014  32.000000  699.000000   \n",
       "avg_word    50000.0     4.640676    0.340731   1.239865    4.417904   \n",
       "stopwords   50000.0    94.768780   71.682687   0.000000   51.000000   \n",
       "upper       50000.0     4.857800    5.627519   0.000000    1.000000   \n",
       "\n",
       "                   50%          75%           max  \n",
       "word_count  173.000000   280.000000   2470.000000  \n",
       "char_count  970.000000  1590.250000  13704.000000  \n",
       "avg_word      4.627006     4.847458     12.290909  \n",
       "stopwords    72.000000   115.000000   1004.000000  \n",
       "upper         3.000000     6.000000    162.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of words\n",
    "data['word_count'] = data['review'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data[['review','word_count']]\n",
    "#Number of characters\n",
    "data['char_count'] = data['review'].str.len() ## this also includes spaces\n",
    "data[['review','char_count']]\n",
    "\n",
    "# Average word length\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "data['avg_word'] = data['review'].apply(lambda x: avg_word(x))\n",
    "data[['review','avg_word']]\n",
    "\n",
    "# Number of stop words \n",
    "stop = stopwords.words('english')\n",
    "data['stopwords'] = data['review'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "# Number of Uppercase words\n",
    "data['upper'] = data['review'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "\n",
    "# Descriptive statistics of the DataFrame\n",
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare analytics here and construct all the data objects you will use in your report.\n",
    "•\tWrite functions and classes to simplify tasks. Do not repeat yourself.\n",
    "\n",
    "•\tAvoid output.\n",
    "\n",
    "•\tRefactor your code until it’s clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tPrint out relevant tables nicely, display well-annotated charts and explain if needed in plain English.\n",
    "•\tUse minimum code here, just output-functions’ calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tConclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tSummarize your findings here in 5...10 lines of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'FindingSimilarItems'...\n",
      "warning: You appear to have cloned an empty repository.\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/AlexTouvras/FindingSimilarItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in .ipynb_checkpoints/Project-checkpoint.ipynb.\n",
      "The file will have its original line endings in your working directory\n",
      "warning: LF will be replaced by CRLF in Project.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    }
   ],
   "source": [
    "#! git add -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main (root-commit) c45f15e] Draft Notebook\n",
      " 2 files changed, 534 insertions(+)\n",
      " create mode 100644 .ipynb_checkpoints/Project-checkpoint.ipynb\n",
      " create mode 100644 Project.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/AlexTouvras/RecurrentNeuralNetworks\n",
      " * [new branch]      main -> main\n"
     ]
    }
   ],
   "source": [
    "! git commit -am \"Draft Notebook\" \n",
    "! git push "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
